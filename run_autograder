#!/usr/bin/env bash
# Gradescope Autograder Run Script
# Runs for EACH student submission

set -uo pipefail

# Directories
SOURCE_DIR="/autograder/source"
SUBMISSION_DIR="/autograder/submission"
RESULTS_DIR="/autograder/results"
WORK_DIR="/autograder/work"

# Create directories
mkdir -p "$RESULTS_DIR"
rm -rf "$WORK_DIR"
mkdir -p "$WORK_DIR"

# Set Java environment
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
export PATH="$JAVA_HOME/bin:$PATH"

echo "=== Gradescope Autograder ==="
echo "Java: $(java -version 2>&1 | head -1)"

# -----------------------------
# Step 1: Set up project structure
# -----------------------------

# Copy pom.xml and test files from autograder source
cp "$SOURCE_DIR/pom.xml" "$WORK_DIR/"
mkdir -p "$WORK_DIR/src/test/java/notifier"
cp -r "$SOURCE_DIR/src/test/java/notifier/"* "$WORK_DIR/src/test/java/notifier/"

# -----------------------------
# Step 2: Copy student submission
# -----------------------------

mkdir -p "$WORK_DIR/src/main/java/notifier"

# Handle different submission formats
if [ -d "$SUBMISSION_DIR/src/main/java/notifier" ]; then
    echo "Found: Maven structure (src/main/java/notifier/)"
    cp -r "$SUBMISSION_DIR/src/main/java/notifier/"* "$WORK_DIR/src/main/java/notifier/"
elif [ -d "$SUBMISSION_DIR/notifier" ]; then
    echo "Found: notifier/ folder"
    cp -r "$SUBMISSION_DIR/notifier/"* "$WORK_DIR/src/main/java/notifier/"
elif ls "$SUBMISSION_DIR"/*.java 1>/dev/null 2>&1; then
    echo "Found: Java files at root"
    cp "$SUBMISSION_DIR"/*.java "$WORK_DIR/src/main/java/notifier/"
else
    echo "Searching for Java files..."
    find "$SUBMISSION_DIR" -name "*.java" -type f -exec cp {} "$WORK_DIR/src/main/java/notifier/" \;
fi

# Count submitted files
FILE_COUNT=$(ls -1 "$WORK_DIR/src/main/java/notifier/"*.java 2>/dev/null | wc -l || echo "0")
echo "Found $FILE_COUNT Java files"

# -----------------------------
# Step 3: Run Maven tests
# -----------------------------

cd "$WORK_DIR"

echo "=== Running mvn clean test ==="
MVN_OUTPUT=$(mvn clean test 2>&1) || true

echo "$MVN_OUTPUT" > "$WORK_DIR/mvn_output.txt"

# -----------------------------
# Step 4: Generate Gradescope JSON
# -----------------------------

python3 << 'PYTHON_SCRIPT'
import json
import os
import xml.etree.ElementTree as ET
from pathlib import Path

# Point values for each test (total = 100)
TEST_POINTS = {
    "testEmailNotifierSendsMessage": 10,
    "testSmsNotifierSendsMessage": 10,
    "testSlackNotifierSendsMessage": 10,
    "testLoggingNotifierLogsBeforeAndAfter": 10,
    "testRetryingNotifierSuccess": 10,
    "testCompositeNotifierSendsToAll": 10,
    "testFakeNotifierRecordsMessage": 10,
    "testSendWelcomeCallsNotifier": 10,
    "testSendWelcomeIncludesUsername": 10,
    "testSendWelcomeIncludesWelcome": 10,
}

def generate_results():
    results = {
        "output": "",
        "output_format": "text",
        "tests": [],
        "score": 0,
        "max_score": 100
    }

    work_dir = Path("/autograder/work")
    report_dir = work_dir / "target" / "surefire-reports"
    mvn_output_file = work_dir / "mvn_output.txt"

    # Read Maven output
    mvn_output = ""
    if mvn_output_file.exists():
        mvn_output = mvn_output_file.read_text()

    # Check for compilation errors
    if "COMPILATION ERROR" in mvn_output or "BUILD FAILURE" in mvn_output:
        error_lines = []
        capture = False
        for line in mvn_output.split('\n'):
            if '[ERROR]' in line:
                capture = True
            if capture:
                error_lines.append(line)
            if len(error_lines) > 20:
                break

        error_msg = '\n'.join(error_lines) if error_lines else "Compilation failed. Check your code for syntax errors."

        results["output"] = "‚ùå Compilation Failed\n\n" + error_msg
        results["tests"].append({
            "name": "Compilation",
            "score": 0,
            "max_score": 100,
            "status": "failed",
            "output": error_msg[:1000]
        })
        return results

    # Parse JUnit XML results
    tests_found = False
    total_score = 0
    passed_count = 0
    failed_count = 0

    if report_dir.exists():
        for xml_file in report_dir.glob("TEST-*.xml"):
            try:
                tree = ET.parse(xml_file)
                root = tree.getroot()

                for testcase in root.findall(".//testcase"):
                    tests_found = True
                    classname = testcase.get("classname", "")
                    name = testcase.get("name", "")
                    display_name = f"{classname.split('.')[-1]}.{name}"

                    # Get point value for this test
                    test_max_score = TEST_POINTS.get(name, 10)

                    failure = testcase.find("failure")
                    error = testcase.find("error")
                    skipped = testcase.find("skipped")

                    if failure is not None:
                        failed_count += 1
                        msg = failure.get("message", "") or failure.text or "Test failed"
                        results["tests"].append({
                            "name": display_name,
                            "score": 0,
                            "max_score": test_max_score,
                            "status": "failed",
                            "output": f"‚ùå {msg[:500]}"
                        })
                    elif error is not None:
                        failed_count += 1
                        msg = error.get("message", "") or error.text or "Test error"
                        results["tests"].append({
                            "name": display_name,
                            "score": 0,
                            "max_score": test_max_score,
                            "status": "failed",
                            "output": f"‚ùå {msg[:500]}"
                        })
                    elif skipped is not None:
                        failed_count += 1
                        results["tests"].append({
                            "name": display_name,
                            "score": 0,
                            "max_score": test_max_score,
                            "status": "failed",
                            "output": "‚ö†Ô∏è Test was skipped"
                        })
                    else:
                        passed_count += 1
                        total_score += test_max_score
                        results["tests"].append({
                            "name": display_name,
                            "score": test_max_score,
                            "max_score": test_max_score,
                            "status": "passed",
                            "output": "‚úÖ Test passed"
                        })
            except Exception as e:
                pass

    if not tests_found:
        results["output"] = "‚ùå No test results found. Make sure your code compiles and all required classes exist."
        results["tests"].append({
            "name": "Test Execution",
            "score": 0,
            "max_score": 100,
            "status": "failed",
            "output": "No tests were executed. Check that your submission includes all required files."
        })
        return results

    # Generate summary
    total_tests = passed_count + failed_count
    if failed_count == 0:
        results["output"] = f"üéâ All {total_tests} tests passed! Great job!\n\nScore: {total_score}/100"
    else:
        results["output"] = f"Your code compiled successfully.\n\n‚úÖ {passed_count}/{total_tests} tests passed\n‚ùå {failed_count}/{total_tests} tests failed\n\nScore: {total_score}/100"

    results["score"] = total_score

    return results

# Generate and write results
results = generate_results()
with open("/autograder/results/results.json", "w") as f:
    json.dump(results, f, indent=2)

print(f"Results: {results['score']}/100 points")
PYTHON_SCRIPT

echo "=== Autograder complete ==="
